# Historical Robots.txt Data Collection and Analysis

## Overview
This project analyzes how online publishers modify their `robots.txt` files over time, specifically focusing on the blocking of AI-based crawlers such as GPTBot, ChatGPT-User, OAI-SearchBot, Google-Extended, PerplexityBot, anthropic-ai, ClaudeBot, and Claude-Web. Using the Wayback Machine, we retrieve and analyze historical snapshots of `robots.txt` files for various publishers.

## Project Structure

- `dataset/`: Contains directories for storing raw and processed data files.
  - robot.ipyb/ : code file 
## Requirements

To run this project, you will need the following Python libraries:
- `requests`
- `pandas`
- `matplotlib`
- `seaborn`
- `waybackpy`
- `numpy`
- `re`
- `datetime`
- `warnings`

Install the required libraries with:
pip install requests pandas matplotlib seaborn waybackpy numpy

